{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://notebooks.gesis.org/binder/v2/gh/jolin-io/fall-in-love-with-julia/main?filepath=10%20SciML%20-%2001%20DiffEqFlux.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.jolin.io\" target=\"_blank\" rel=\"noreferrer noopener\">\n",
    "<img src=\"https://www.jolin.io/assets/Jolin/Jolin-Banner-Website-v1.1-darkmode.webp\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fall-in-love-with-Julia: SciML and DiffEqFlux.jl\n",
    "\n",
    "a 101 introduction session\n",
    "\n",
    "\n",
    "I am Stephan Sahm, and here is what we are going to learn today\n",
    "\n",
    "1. Flux.jl - Neural Networks\n",
    "2. DifferentialEquations.jl\n",
    "3. SciML - DiffEq within NeuralNet\n",
    "4. SciML - NeuralNet within DiffEq\n",
    "5. Turing.jl - Uncertainity Modelling via Bayesian Estimation\n",
    "6. SciML - Bayesian DiffEq\n",
    "7. SciML - DataDrivenDiffEq.jl and Symbolic Regression\n",
    "\n",
    "\n",
    "The main sources of information for this jupyter notebook are the following two tutorials\n",
    "- [Blog Post DiffEqFlux.jl](https://julialang.org/blog/2019/01/fluxdiffeq/)\n",
    "- [Tutorial Bayesian Differential Equations](https://turing.ml/dev/tutorials/10-bayesian-differential-equations/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using StatsPlots\n",
    "using Statistics\n",
    "using Random\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flux.jl - Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function goal(x)\n",
    "    s = sum(x)\n",
    "    s > length(x)/2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [rand(10) for i in 1:1000];\n",
    "train_y = goal.(train_x);\n",
    "\n",
    "val_x = [rand(10) for i in 1:100];\n",
    "val_y = goal.(val_x);\n",
    "\n",
    "p1 = histogram(sum.(train_x)[train_y.==0], label=:0, color=:red)\n",
    "p2 = histogram(sum.(train_x)[train_y.==1], label=:1, color=:green)\n",
    "plot(p1, p2, layout=(2,1), link=:x, xlabel=:sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = Flux.Chain(\n",
    "    x -> x.^3,\n",
    "    Flux.Dense(10 => 5, Flux.Ïƒ),\n",
    "    Flux.Dense(5 => 2),\n",
    "    Flux.softmax\n",
    ")\n",
    "neural_net(rand(10)) # => 2-element vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_accuracy(x, y) = Flux.onecold(neural_net(x), 0:1) == y\n",
    "accuracy(x, y) = mean(_accuracy.(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(train_x, train_y), accuracy(val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(x, y) = Flux.crossentropy(neural_net(x), Flux.onehot(y, 0:1))\n",
    "opt = Flux.Momentum(0.01)\n",
    "params = Flux.params(neural_net)\n",
    "\n",
    "for (x, y) in zip(train_x, train_y)\n",
    "    gs = Flux.gradient(params) do\n",
    "        loss(x, y)\n",
    "    end\n",
    "    Flux.update!(opt, params, gs)\n",
    "end\n",
    "accuracy(train_x, train_y), accuracy(val_x, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ«µ It is your time:** create a plot similar to the histogram above in order to visualize how well our neural network worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DifferentialEquations.jl\n",
    "\n",
    "Example [Lotka-Volterra equations](https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations): Population of rabbits and foxes\n",
    "\n",
    "<center>\n",
    "\n",
    "rabbits: $ x^\\prime = \\alpha x - \\beta x y $\n",
    "\n",
    "</center>\n",
    "\n",
    "the rate of change of the prey's population is given by its own growth rate ($\\alpha$) minus the rate at which it is preyed upon ($\\beta$).\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "foxes: $ y^\\prime = \\gamma x y - \\delta y $\n",
    "\n",
    "</center>\n",
    "\n",
    "the rate of change of the predator's population depends upon the rate at which it consumes prey ($\\gamma$), minus its intrinsic death rate ($\\delta$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DifferentialEquations\n",
    "function lotka_volterra(du, u, p, t)\n",
    "    x, y = u\n",
    "    Î±, Î², Î´, Î³ = p\n",
    "    du[1] = dx = Î±*x - Î²*x*y\n",
    "    du[2] = dy = -Î´*y + Î³*x*y\n",
    "end\n",
    "u0 = [1.0, 1.0]\n",
    "tspan = (0.0, 10.0)\n",
    "p = [1.5, 1.0, 3.0, 1.0]\n",
    "ode_prob = ODEProblem(lotka_volterra, u0, tspan, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode_sol = solve(ode_prob)\n",
    "plot(ode_sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciML - DiffEq within NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Flux, DiffEqFlux\n",
    "using DifferentialEquations\n",
    "\n",
    "p = [2.2, 1.0, 2.0, 0.4] # Initial Parameter Vector\n",
    "params = Flux.params(p)\n",
    "function predict_rd() # Our 1-layer \"neural network\"\n",
    "    # override with new parameters\n",
    "    solve(ode_prob, Tsit5(), p=p, saveat=0.1)[1,:]\n",
    "end\n",
    "loss_rd() = sum(abs2, x-1 for x in predict_rd())\n",
    "\n",
    "\n",
    "data = Iterators.repeated((), 100)\n",
    "opt = Flux.ADAM(0.1)\n",
    "cb = function () #callback function to observe training\n",
    "    # display(loss_rd())\n",
    "    # using `remake` to re-create our `ode_prob` with current parameters `p`\n",
    "    sleep(0.05)\n",
    "    plot(solve(remake(ode_prob,p=p), Tsit5(), saveat=0.1), ylim=(0,6), show=:inline)\n",
    "end\n",
    "\n",
    "# Display the ODE with the initial parameter values.\n",
    "cb();\n",
    "Flux.train!(loss_rd, params, data, opt, cb=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an ODE can be part of a larger neural network\n",
    "```julia \n",
    "neuralnet = Chain(\n",
    "    Dense(28^2, 32, relu),\n",
    "    # requires an ODE of 32 params\n",
    "    p -> solve(ode_prob, Tsit5(), p=p,\n",
    "               saveat=0.1)[1,:],\n",
    "    Dense(32, 10),\n",
    "    softmax)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ«µ It is your time:** create data from an alternative Lotka Volterra system and fit it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciML - NeuralNet within DiffEq\n",
    "\n",
    "Ground Truth $u^\\prime = A u^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DifferentialEquations\n",
    "\n",
    "function trueODEfunc(du,u,p,t)\n",
    "    true_A = [-0.1 -2.0\n",
    "               2.0 -0.1]\n",
    "    du .= true_A * u.^3\n",
    "end\n",
    "\n",
    "u0 = Float32[2.; 0.]\n",
    "datasize = 30\n",
    "tspan = (0.0f0, 1.5f0)\n",
    "\n",
    "t = range(tspan[1],tspan[2],length=datasize)\n",
    "ode2_prob = ODEProblem(trueODEfunc,u0,tspan)\n",
    "ode2_sol = solve(ode2_prob,Tsit5(),saveat=t)\n",
    "ode2_data = Array(ode2_sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model $u^\\prime$ with neural network.\n",
    "(multilayer perceptron with 1 hidden layer and a tanh activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Flux, DiffEqFlux\n",
    "dudt = Flux.Chain(\n",
    "    x -> x.^3,\n",
    "    Flux.Dense(2, 50, tanh),\n",
    "    Flux.Dense(50, 2),\n",
    ")\n",
    "\n",
    "n_ode = DiffEqFlux.NeuralODE(dudt, tspan, Tsit5(), saveat=t,\n",
    "                  reltol=1e-7, abstol=1e-9)\n",
    "ps = Flux.params(n_ode)\n",
    "\n",
    "# Get the prediction using the correct initial condition\n",
    "pred = n_ode(u0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(t, ode2_data[1,:], label=\"data\")\n",
    "scatter!(t, pred[1,:], label=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_n_ode() = sum(abs2, ode2_data .- n_ode(u0))\n",
    "loss_n_ode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Iterators.repeated((), 100)\n",
    "opt = Flux.ADAM(0.1)\n",
    "cb = function () #callback function to observe training\n",
    "    # plot current prediction against data\n",
    "    cur_pred = n_ode(u0)\n",
    "    pl = scatter(t, ode2_data[1,:], label=\"data\")\n",
    "    scatter!(pl, t, cur_pred[1,:], label=\"prediction\")\n",
    "    plot(pl, show=:inline)\n",
    "end\n",
    "\n",
    "# Display the ODE with the initial parameter values.\n",
    "cb()\n",
    "Flux.train!(loss_n_ode, ps, data, opt, cb = cb)\n",
    "loss_n_ode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ«µ It is your time:** try fitting Lotka Volterra data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative perspective: Paper Neural Ordinary Differential Equations (Chen et al. 2019)\n",
    "\n",
    "Residual Neural Network (discrete difference layers)\n",
    "$$h_{t+1} = h_t + f(h_t, \\theta_t)$$\n",
    "\n",
    "Neural Ordinary Differential Equations\n",
    "$$\\frac{dh(t)}{dt} = f(h(t), t, \\theta)$$\n",
    "\n",
    "![](https://www.jolin.io/assets/examples/NeuralODE-Comparing-ResNet.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turing.jl - Uncertainty Modelling via Bayesian Estimation\n",
    "\n",
    "Coin Flip mini example\n",
    "\n",
    "find more details at https://turing.ml/dev/tutorials/00-introduction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Turing\n",
    "\n",
    "# Set the true probability of heads in a coin.\n",
    "p_true = 0.5\n",
    "\n",
    "# Iterate from having seen 0 observations to 100 observations.\n",
    "N = 100\n",
    "\n",
    "# Draw data from a Bernoulli distribution, i.e. draw heads or tails.\n",
    "Random.seed!(12)\n",
    "data = rand(Bernoulli(p_true), N)\n",
    "\n",
    "# Declare our Turing model.\n",
    "@model function coinflip(y)\n",
    "    # Our prior belief about the probability of heads in a coin.\n",
    "    p ~ Beta(1, 1)\n",
    "\n",
    "    # The number of observations.\n",
    "    yN = length(y)\n",
    "    for n in 1:yN\n",
    "        # Heads or tails of a coin are drawn from a Bernoulli distribution.\n",
    "        y[n] ~ Bernoulli(p)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Settings of the Hamiltonian Monte Carlo (HMC) sampler.\n",
    "iterations = 1000\n",
    "Ïµ = 0.05\n",
    "Ï„ = 10\n",
    "\n",
    "# Start sampling.\n",
    "chain = sample(coinflip(data), HMC(Ïµ, Ï„), iterations)\n",
    "plot(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(chain[:p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ«µ It is your time:** Try different `N` above and see how our information about `p` improves/worsens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciML - Bayesian DiffEq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume noisy Lotka Volterra data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode_sol = solve(ode_prob, Tsit5(), saveat=0.1)\n",
    "ode_data = (Array(ode_sol) + 0.8\n",
    "           * randn(size(Array(ode_sol))))\n",
    "# Plot simulation & noisy observations\n",
    "plot(ode_sol, alpha=0.3)\n",
    "scatter!(ode_sol.t, ode_data',\n",
    "         color=[1 2], label=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we only have predator-data (foxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Turing\n",
    "using DifferentialEquations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function fitlv(data::AbstractVector, ode_prob)\n",
    "    # Prior distributions.\n",
    "    Î± ~ truncated(Normal(1.5, 0.5), 0.5, 2.5)\n",
    "    Î² ~ truncated(Normal(1.2, 0.5), 0, 2)\n",
    "    Î³ ~ truncated(Normal(3.0, 0.5), 1, 4)\n",
    "    Î´ ~ truncated(Normal(1.0, 0.5), 0, 2)\n",
    "    p = [Î±, Î², Î³, Î´]\n",
    "    \n",
    "    # Simulate Lotka-Volterra model but save only\n",
    "    # the second state of the system (predators).\n",
    "    predicted = solve(ode_prob, Tsit5(), p=p, saveat=0.1, save_idxs=2)\n",
    "    \n",
    "    # Observations of the predators.\n",
    "    Ïƒ ~ InverseGamma(2, 3)\n",
    "    data ~ MvNormal(predicted.u, Ïƒ^2 * I)\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "# fit model only to predators (foxes)\n",
    "model = fitlv(ode_data[2, :], ode_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample & plot (called data retroduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 3 independent chains.\n",
    "chain = sample(model, NUTS(0.45), MCMCSerial(), 5000, 3, progress=false)\n",
    "posterior_samples = sample(chain[[:Î±, :Î², :Î³, :Î´]], 300, replace=false)\n",
    "\n",
    "plot(legend=false)\n",
    "for p in eachrow(Array(posterior_samples))\n",
    "    ode_sol_p = solve(ode_prob, Tsit5(), p=p, saveat=0.1)\n",
    "    plot!(ode_sol_p, alpha=0.1, color=\"#BBBBBB\")\n",
    "end\n",
    "\n",
    "# Plot simulation and noisy observations.\n",
    "plot!(ode_sol, color=[1 2], linewidth=1)\n",
    "scatter!(ode_sol.t, ode_data', color=[1 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ«µ It is your time:** How can we check whether the MCMC Bayesian Estimation converged successfully?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciML - DataDrivenDiffEq.jl and Symbolic Regression\n",
    "\n",
    "Extract human readable formula from data via symbolic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataDrivenDiffEq\n",
    "using ModelingToolkit\n",
    "\n",
    "f(u) = u.^2 .+ 2.0u .- 1.0\n",
    "X = randn(1, 100);\n",
    "Y = f.(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@named problem = DirectDataDrivenProblem(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@variables u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_eqs = monomial_basis([u], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [u]\n",
    "basis = Basis(basis_eqs, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = solve(problem, basis, STLSQ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Material\n",
    "\n",
    "- [Tutorial Deep Learning with Flux - A 30-minutes Blitz](https://fluxml.ai/tutorials/2020/09/15/deep-learning-flux.html)\n",
    "- [Blog Post DiffEqFlux.jl](https://julialang.org/blog/2019/01/fluxdiffeq/)\n",
    "- [Documentation DiffEqFlux.jl](https://diffeqflux.sciml.ai/stable/)\n",
    "- [Paper Neural Ordinary Differential Equations (Chen et al. 2019)](https://arxiv.org/abs/1806.07366)\n",
    "- [Paper Universal Differential Equations for SciML (Rackauckas et al. 2020)](https://arxiv.org/abs/2001.04385)\n",
    "- [Tutorial Bayesian Differential Equations](https://turing.ml/dev/tutorials/10-bayesian-differential-equations/)\n",
    "- [Documentation DataDrivenDiffEq.jl](https://datadriven.sciml.ai/stable), [linear ODE example](https://datadriven.sciml.ai/stable/examples/2_linear_continuous_system/), [nonlinear ODE example](https://datadriven.sciml.ai/stable/examples/4_nonlinear_continuous_system/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you for joining\n",
    "\n",
    "for questions or suggestions please contact me at stephan.sahm@jolin.io\n",
    "\n",
    "\n",
    "#### Sponsored by [Jolin.io](https://www.jolin.io)\n",
    "\n",
    "Jolin.io is an IT-consultancy focussing on Julia\n",
    "\n",
    "We are there to help you, if you want to\n",
    "- try out Julia at your company, or\n",
    "- transition Matlab, Fortran, R, Python, etc. to Julia\n",
    "- or speed up your existing Julia code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.jolin.io\" target=\"_blank\" rel=\"noreferrer noopener\">\n",
    "<img src=\"https://www.jolin.io/assets/Jolin/Jolin-Banner-Website-v1.1-darkmode.webp\">\n",
    "</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
